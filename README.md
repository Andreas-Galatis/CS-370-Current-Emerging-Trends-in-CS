# CS-370-Current-Emerging-Trends-in-CS

In the Treasure Hunt game algorithm, a Deep Q-Learning (DQN) technique of Reinforcement Learning (RL) was used for the agent to choose the best action for each given circumstance. RL algorithms work by incentivizing the agent with rewards for correct actions and punishments for bad ones. There is also an element of randomness in the actions that the agent takes in order to explore other possible avenues, sort of like an advanced method of trial and error. This is considered to be a value-based approach to RL, as the DQN network, which is based on the Bellman equation, optimizes data collected at any point of the training regardless of how the agent chooses to explore the environment when the data is obtained (Surma, 2021).

This kind of reinforcement learning is not that much different in concept from the way humans also learn. From the moment of birth, we as humans are constantly undergoing learning and training on how to best function in our world environment. Sometimes we learn by being taught, and sometimes we learn by experience. Similarly, RL algorithms employ two methods to training called the policy-based approach and the value-based approach. The value-based approach is the one that was used in the Treasure Hunt Game, learning by trial and error, and a policy-based approach consists of providing the agent with a set of rules beforehand to train it on the best course of actions. The policy-based approach is considered more reliable, especially for instance where a specific target is desired, as the optimization methods are based on a set of principles. The value-based approach, on the other hand, requires less data since it reuses its own findings over and over again, but this also makes it prone to more failure modes due to more exploration (A Taxonomy of RL Algorithms, 2018).
  
In RL, the human concept of doing things based on experience, or things we are comfortable with, as opposed to trying out new things and being adventurous, is called exploitation and exploration. Exploitation is known as the greedy approach where agents attempt to get more rewards by performing actions that are already known to have higher estimated values, essentially making decisions based on existing data. Exploration is different from exploitation as it mostly focuses on getting new knowledge rather than relying on existing data, therefore aiming for better long-term results instead of immediate ones (Yang, 2022). The question of how much exploration versus exploitation should be conducted is an ongoing dilemma, not only in AI, but in real life as well. Should someone simply stick to what they know and not explore what possibilities lie in new endeavors, or should they constantly aim for something new only to discover that they had the best method figured out already. The same question is encapsulated in the discussion between exploration and exploitation in RL as well. There are a couple of solutions to address this dilemma and the most common and simplest one is known as the Epsilon-Greedy algorithm. In this algorithm the machine is instructed to begin with exploration and over time use exploitation more and more as higher known reward values are discovered. By setting the exploration factor to about ten percent the machine will mostly return to the highest rewards, but from time to time will also select a random action to explore. This way the machine will eventually optimize its options and produce better results (Parkinson, 2021). 
	
Using DQN with neural networks is a great way to solve complex problems and even some fun ones like playing a complicated game such as chess, or a simple maze game like the Treasure Hunt game in this project. The Epsilon-Greedy algorithm was used in the Treasure Hunt game as the agent learned mostly through exploitation, but from time-to-time used exploration to discover new paths. In a maze comprised on an 8x8 matrix, the agent was able to find the treasure and reach a 100% win-rate after about 118 epochs, which took it just a little over 18 minutes to do so. What’s interesting is that although there were different paths that the agent could’ve taken to accomplish the goal, once it discovered a path that works it stuck to it. This is where the trade-off between exploitation and exploration happens. The goal was not to find all possible routes, but to get the treasure. With each epoch the win count increased until the agent was fully optimized and able to reach a consistent win rate of 100% in each episode. 
  
**References**

A Taxonomy of RL Algorithms. (2018). Open AI. Retrieved December 4, 2022, from https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html 
Surma, G. (2021, October 13). Cartpole - Introduction to Reinforcement Learning (DQN - Deep Q-Learning). Medium. https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288 

Parkinson, A. (2021, December 12). The Epsilon-Greedy Algorithm for Reinforcement Learning. Medium. https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870 

Yang, A. (2022, July 25). What is Exploration vs. Exploitation in Reinforcement Learning? Medium. https://medium.com/@angelina.yang/what-is-exploration-vs-exploitation-in-reinforcement-learning-a3b96dcc9503 

